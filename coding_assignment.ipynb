{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSMuCaBzkBzAZIpFulkGd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merwinjos/AutoAnalysis/blob/master/coding_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1pdILKITGH7J",
        "outputId": "fe22cbd5-a935-4efb-92ca-f92292c1be19"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'fetch_canoo_info_wikipedia' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-8e8dd88f7c33>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mcanoo_info_wikipedia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_canoo_info_wikipedia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mcanoo_info_macroaxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_canoo_info_macroaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fetch_canoo_info_wikipedia' is not defined"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "def fetch_canoo_info():\n",
        "    url = 'https://en.wikipedia.org/wiki/Canoo'\n",
        "\n",
        "    # Sending an HTTP request\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parsing the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extracting relevant information\n",
        "    company_name = soup.find(class_='mw-page-title-main').text.strip()\n",
        "    company_history = soup.find('div', class_='mw-content-ltr mw-parser-output').text.strip()\n",
        "    #company_compitators= soup.find('div', class_='ui segment').text.strip()\n",
        "\n",
        "    # You'll need to inspect the Wikipedia page to find the appropriate HTML tags/classes\n",
        "    # and extract relevant information accordingly.\n",
        "    # For example:\n",
        "    # company_summary = soup.find('p').text.strip()\n",
        "    # Add more data points as needed\n",
        "\n",
        "    return {\n",
        "        'Company Name': company_name,\n",
        "        'Company history': company_history,\n",
        "        #'company compitators': company_compitators,\n",
        "        # Add more keys for additional data points\n",
        "    }\n",
        "\n",
        "\n",
        "    def fetch_canoo_info_macroaxis():\n",
        "     macroaxis_url = 'https://www.macroaxis.com/competition/GOEV'\n",
        "\n",
        "    # Sending an HTTP request\n",
        "    response = requests.get(macroaxis_url)\n",
        "\n",
        "    # Parsing the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extracting relevant information\n",
        "    # Example:\n",
        "    company_competition = soup.find('div', class_='ui segment').text.strip()\n",
        "    # Add more data points as needed\n",
        "\n",
        "    return {\n",
        "        'Competition': company_competition,\n",
        "        # Add more keys for additional data points\n",
        "    }\n",
        "\n",
        "def save_to_xml(data, filename='canoo_info.xml'):\n",
        "    root = ET.Element('Company')\n",
        "    for key, value in data.items():\n",
        "        ET.SubElement(root, key.replace(' ', '_')).text = value\n",
        "\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(filename, encoding='utf-8', xml_declaration=True)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    canoo_info_wikipedia = fetch_canoo_info_wikipedia()\n",
        "    canoo_info_macroaxis = fetch_canoo_info_macroaxis()\n",
        "\n",
        "    combined_info = {**canoo_info_wikipedia, **canoo_info_macroaxis} # Combine information from both sources\n",
        "\n",
        "    save_to_xml(combined_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_canoo_info_macroaxis():\n",
        "     macroaxis_url = 'https://www.macroaxis.com/competition/GOEV'\n",
        "\n",
        "    # Sending an HTTP request\n",
        "    response = requests.get(macroaxis_url)\n",
        "\n",
        "    # Parsing the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extracting relevant information\n",
        "    # Example:\n",
        "    company_competition = soup.find('div', class_='ui segment').text.strip()\n",
        "    # Add more data points as needed\n",
        "\n",
        "    return {\n",
        "        'Competition': company_competition,\n",
        "        # Add more keys for additional data points\n",
        "    }"
      ],
      "metadata": {
        "id": "zWZu5fS9fwgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBgfc8mifxdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "def fetch_canoo_info():\n",
        "    url = 'https://en.wikipedia.org/wiki/Canoo'\n",
        "\n",
        "    # Sending an HTTP request\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parsing the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extracting relevant information\n",
        "    company_name = soup.find(class_='mw-page-title-main').text.strip()\n",
        "    company_history = soup.find('div', class_='mw-content-ltr mw-parser-output').text.strip()\n",
        "    #company_compitators= soup.find('div', class_='ui segment').text.strip()\n",
        "\n",
        "    # You'll need to inspect the Wikipedia page to find the appropriate HTML tags/classes\n",
        "    # and extract relevant information accordingly.\n",
        "    # For example:\n",
        "    # company_summary = soup.find('p').text.strip()\n",
        "    # Add more data points as needed\n",
        "\n",
        "    return {\n",
        "        'Company Name': company_name,\n",
        "        'Company history': company_history,\n",
        "        #'company compitators': company_compitators,\n",
        "        # Add more keys for additional data points\n",
        "    }\n",
        "\n",
        "\n",
        "    def fetch_canoo_info_macroaxis():\n",
        "     url = 'https://www.macroaxis.com/competition/GOEV'\n",
        "\n",
        "    # Sending an HTTP request\n",
        "    response = requests.get(macroaxis_url)\n",
        "\n",
        "    # Parsing the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extracting relevant information\n",
        "    # Example:\n",
        "    company_competition = soup.find('div', class_='ui segment').text.strip()\n",
        " def save_to_xml(data, filename='canoo_info.xml'):\n",
        "    root = ET.Element('Company')\n",
        "    for key, value in data.items():\n",
        "        ET.SubElement(root, key.replace(' ', '_')).text = value\n",
        "\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(filename, encoding='utf-8', xml_declaration=True)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    canoo_info = fetch_canoo_info()\n",
        "    save_to_xml(canoo_info)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "u8YWGPJdeh_t",
        "outputId": "3063fbf8-2c60-4dee-aff9-9b2f97d03f36"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 45)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    def save_to_xml(data, filename='canoo_info.xml'):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "def fetch_canoo_info_wikipedia():\n",
        "    wikipedia_url = 'https://en.wikipedia.org/wiki/Canoo'\n",
        "\n",
        "    # Sending an HTTP request\n",
        "    response = requests.get(wikipedia_url)\n",
        "\n",
        "    # Parsing the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    company_name = soup.find(class_='mw-page-title-main').text.strip()\n",
        "    company_history = soup.find('div', class_='mw-content-ltr mw-parser-output').text.strip()\n",
        "    #company_compitators= soup.find('div', class_='ui segment').text.strip()\n",
        "\n",
        "    # You'll need to inspect the Wikipedia page to find the appropriate HTML tags/classes\n",
        "    # and extract relevant information accordingly.\n",
        "    # For example:\n",
        "    # company_summary = soup.find('p').text.strip()\n",
        "    # Add more data points as needed\n",
        "\n",
        "    return {\n",
        "        'Company Name': company_name,\n",
        "        'Company history': company_history,\n",
        "        #'company compitators': company_compitators,\n",
        "        # Add more keys for additional data points\n",
        "    }\n",
        "\n",
        "def fetch_canoo_info_globaldata():\n",
        "    globaldata_url = 'https://www.globaldata.com/store/report/canoo-inc-company-profile/'\n",
        "\n",
        "    # Sending an HTTP request\n",
        "    response = requests.get(globaldata_url)\n",
        "\n",
        "    # Parsing the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extracting relevant information\n",
        "    # Example:\n",
        "    company_competition = soup.find('div', class_='cell medium-8 large-9').text.strip()\n",
        "    # Add more data points as needed\n",
        "\n",
        "    return {\n",
        "        'Competition': company_competition,\n",
        "        # Add more keys for additional data points\n",
        "    }\n",
        "def fetch_canoo_info_report():\n",
        "    report_url = 'https://investors.canoo.com/financial-information/income-statement'\n",
        "\n",
        "    # Sending an HTTP request\n",
        "    response = requests.get(report_url)\n",
        "\n",
        "    # Parsing the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "    # Extracting relevant information\n",
        "    # Example:\n",
        "    report_url = soup.find('div', class_='main-content').text.strip()\n",
        "    # Add more data points as needed\n",
        "\n",
        "    return {\n",
        "        'report': report_url,\n",
        "\n",
        "    }\n",
        "def save_to_xml(data, filename='canoo_info.xml'):\n",
        "    root = ET.Element('Company')\n",
        "    for key, value in data.items():\n",
        "        ET.SubElement(root, key.replace(' ', '_')).text = value\n",
        "\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(filename, encoding='utf-8', xml_declaration=True)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    canoo_info_wikipedia = fetch_canoo_info_wikipedia()\n",
        "    canoo_info_globaldata = fetch_canoo_info_globaldata()\n",
        "    canoo_info_report = fetch_canoo_info_report()\n",
        "\n",
        "    combined_info = {**canoo_info_wikipedia, **fetch_canoo_info_globaldata}\n",
        "    save_to_xml(combined_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "flMXPfTzfyuM",
        "outputId": "4a8b012b-d4ab-4708-9f27-5bfd57e6c995"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'function' object is not a mapping",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-6050d3c543d7>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mcanoo_info_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_canoo_info_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mcombined_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcanoo_info_wikipedia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfetch_canoo_info_globaldata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0msave_to_xml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'function' object is not a mapping"
          ]
        }
      ]
    }
  ]
}